from fastapi import FastAPI, BackgroundTasks, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from supabase import create_client
import logging
import os
import sys
import time
from typing import List
from pipeline import pipeline  # direct import

# ---------------------------
# CONFIG
# ---------------------------
SUPABASE_URL = "https://uxrdywchpcwljsteomtn.supabase.co"
SUPABASE_KEY = "YOUR_SUPABASE_KEY"  # replace with secure env var in production
OUTPUT_DIR = os.path.join(os.path.dirname(__file__), "output")
STORAGE_BUCKET = "sentinel_reports"

os.makedirs(OUTPUT_DIR, exist_ok=True)

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("sentinel-api")

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ---------------------------
# MODELS
# ---------------------------

class CustomRun(BaseModel):
    platforms: List[str] = ["manifold", "polymarket"]
    min_liquidity: float = 500
    max_hours: int = 720
    extra_args: List[str] = []

# ---------------------------
# HELPER FUNCTIONS
# ---------------------------

def upload_csv_to_supabase(job_id: str, filepath: str) -> str:
    storage_filename = f"{job_id}_{os.path.basename(filepath)}"
    with open(filepath, "rb") as f:
        supabase.storage.from_(STORAGE_BUCKET).upload(storage_filename, f, file_options={"content-type": "text/csv"})
    return f"{STORAGE_BUCKET}/{storage_filename}"

def mark_job_status(job_id: str, status: str, storage_path: str = None, error: str = None):
    update_data = {"status": status}
    if storage_path:
        update_data["storage_path"] = storage_path
    if error:
        update_data["error"] = error
    supabase.table("pipeline_jobs").update(update_data).eq("id", job_id).execute()

# ---------------------------
# PIPELINE RUNNER
# ---------------------------

def run_tracked_pipeline(job_id: str, config: CustomRun):
    logger.info(f"[JOB:{job_id}] Starting pipeline for platforms: {config.platforms}")
    mark_job_status(job_id, "running")

    try:
        # Call the pipeline directly instead of subprocess
        pipeline.main(
            platforms=config.platforms,
            min_liquidity=config.min_liquidity,
            max_hours=config.max_hours,
            output_dir=OUTPUT_DIR
        )

        # Find latest CSV
        csv_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith(".csv")]
        if not csv_files:
            raise RuntimeError("No CSV output generated by pipeline")

        csv_files.sort(key=lambda x: os.path.getctime(os.path.join(OUTPUT_DIR, x)), reverse=True)
        latest_file = os.path.join(OUTPUT_DIR, csv_files[0])

        # Upload to Supabase storage
        storage_path = upload_csv_to_supabase(job_id, latest_file)
        mark_job_status(job_id, "completed", storage_path=storage_path)
        logger.info(f"[JOB:{job_id}] Completed successfully. Stored at: {storage_path}")

    except Exception as e:
        logger.error(f"[JOB:{job_id}] Failed: {e}")
        mark_job_status(job_id, "failed", error=str(e))

# ---------------------------
# ROUTES
# ---------------------------

@app.post("/custom")
async def run_custom(config: CustomRun, background_tasks: BackgroundTasks):
    if not config.platforms:
        raise HTTPException(status_code=400, detail="Platforms list cannot be empty")

    job = supabase.table("pipeline_jobs").insert({"status": "pending", "command": str(config)}).execute()
    job_id = job.data[0]["id"]

    background_tasks.add_task(run_tracked_pipeline, job_id, config)
    return {"job_id": job_id, "status": "started", "platforms": config.platforms}

@app.get("/jobs")
async def get_jobs():
    result = supabase.table("pipeline_jobs").select("*").order("created_at", desc=True).execute()
    return result.data

@app.get("/health")
def health():
    return {"status": "ok"}

@app.post("/update")
async def run_default_update(background_tasks: BackgroundTasks):
    config = CustomRun(
        platforms=["manifold", "polymarket"],
        min_liquidity=1155,
        max_hours=30000,
        extra_args=[]
    )

    job = supabase.table("pipeline_jobs").insert({"status": "pending", "command": str(config)}).execute()
    job_id = job.data[0]["id"]

    background_tasks.add_task(run_tracked_pipeline, job_id, config)
    return {"job_id": job_id, "status": "started", "platforms": config.platforms}

# ---------------------------
# RUN SERVER
# ---------------------------
if __name__ == "__main__":
    import uvicorn
    port = int(os.environ.get("PORT", 8000))
    uvicorn.run("main:app", host="0.0.0.0", port=port)
